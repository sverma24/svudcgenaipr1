{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a226087a-e2d6-4def-b33c-47e7d9a114fa",
   "metadata": {},
   "source": [
    "# Project: Apply Lightweight Fine-Tuning to a Foundation Model\n",
    "\n",
    "In this project, have used the Emotions dataset [https://huggingface.co/datasets/dair-ai/emotion]\n",
    "\n",
    "The dataset contains:\n",
    "- Text column which has statements that need to be classified as one of the emotions\n",
    "- Label column which has labels describing the emotion as sadness (0), joy (1), love (2), anger (3), fear (4), surprise (5)  \n",
    "\n",
    "Initially the foundation model BERT sentiment classifier (DistilBERT) will be used, without finetuning the base model parameters and then its performance will be evaluated. Then, the model will be trained using PEFT approach to finetune the parameters and the performance will be evaluated. Finally, we will compare the performance of the model without parameter finetuning and with parameter efficient finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf66c72-aab1-4ab3-922b-85f40ae726a5",
   "metadata": {},
   "source": [
    "### Importing Libraries and Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f32916fb-c265-44ec-9785-d0821b1acf73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb13b68-7bfa-4610-ace5-01258a421f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 16000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the split configuration of the Emotion dataset\n",
    "dataset = load_dataset(\"emotion\", \"split\")\n",
    "\n",
    "# View the dataset characteristics\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df8bad3-ee73-49f4-ad90-a917cea605ab",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894547a0-1b32-4beb-b231-adf837e4ee1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravverma/anaconda3/envs/udacitygenai/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'i didnt feel humiliated', 'label': 0, 'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Inspect the tokenized dataset\n",
    "print(tokenized_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8729228-113e-4f21-9a44-80c567ccd626",
   "metadata": {},
   "source": [
    "### Load and Setup the model without finetuning the base model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "877a5167-85dc-4a33-af4c-6ae86e6d3b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=6, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained model for sequence classification\n",
    "model_nofinetuning = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=6,\n",
    "    id2label={0: \"sadness\", 1: \"joy\", 2: \"love\", 3: \"anger\", 4: \"fear\", 5: \"surprise\"},\n",
    "    label2id={\"sadness\": 0, \"joy\": 1, \"love\": 2, \"anger\": 3, \"fear\": 4, \"surprise\": 5},\n",
    ")\n",
    "\n",
    "#Freeze all the parameters of base model\n",
    "for param in model_nofinetuning.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_nofinetuning.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe8f559b-f7d0-4c0b-bce2-45193228ed78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_nofinetuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6979ba5b-26f1-487c-8cd4-eb70f631a39d",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ea80e61-d01c-4e18-a41e-9614ace0c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy Metric to be supplied to trainer\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15fb1405-bd80-4984-bcb0-5e97c5d01e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravverma/anaconda3/envs/udacitygenai/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 00:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.568200</td>\n",
       "      <td>1.539479</td>\n",
       "      <td>0.448500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.515900</td>\n",
       "      <td>1.507341</td>\n",
       "      <td>0.485000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.501300</td>\n",
       "      <td>1.497092</td>\n",
       "      <td>0.487500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=1.539314208984375, metrics={'train_runtime': 53.9177, 'train_samples_per_second': 890.246, 'train_steps_per_second': 13.91, 'total_flos': 703485182771712.0, 'train_loss': 1.539314208984375, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Trainer\n",
    "trainer_nofinetuning = Trainer(\n",
    "    model=model_nofinetuning,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/emotions\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=64,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy = \"epoch\",\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        logging_dir='.data/emotions/logs_no_finetuning',\n",
    "        logging_steps=100,\n",
    "        ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer_nofinetuning.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d885ba91-7649-4879-8c72-2907240b9ff7",
   "metadata": {},
   "source": [
    "### Evaluate the Model without Finetuning and visualize its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03bcee07-b4aa-405c-8f42-d98bfb7691d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.4970916509628296,\n",
       " 'eval_accuracy': 0.4875,\n",
       " 'eval_runtime': 1.4719,\n",
       " 'eval_samples_per_second': 1358.783,\n",
       " 'eval_steps_per_second': 21.741,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_nofinetuning.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d943df7-d1e0-484c-8e20-578f25277892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>True Label</th>\n",
       "      <th>Predicted Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im feeling rather rotten so im not very ambiti...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im updating my blog because i feel shitty</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i never make her separate from me because i do...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i left with my bouquet of red and yellow tulip...</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was feeling a little vain when i did this one</td>\n",
       "      <td>sadness</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i cant walk into a shop anywhere where i do no...</td>\n",
       "      <td>fear</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i felt anger when at the end of a telephone call</td>\n",
       "      <td>anger</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i explain why i clung to a relationship with a...</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i like to have the same breathless feeling as ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i jest i feel grumpy tired and pre menstrual w...</td>\n",
       "      <td>anger</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text True Label  \\\n",
       "0  im feeling rather rotten so im not very ambiti...    sadness   \n",
       "1          im updating my blog because i feel shitty    sadness   \n",
       "2  i never make her separate from me because i do...    sadness   \n",
       "3  i left with my bouquet of red and yellow tulip...        joy   \n",
       "4    i was feeling a little vain when i did this one    sadness   \n",
       "5  i cant walk into a shop anywhere where i do no...       fear   \n",
       "6   i felt anger when at the end of a telephone call      anger   \n",
       "7  i explain why i clung to a relationship with a...        joy   \n",
       "8  i like to have the same breathless feeling as ...        joy   \n",
       "9  i jest i feel grumpy tired and pre menstrual w...      anger   \n",
       "\n",
       "  Predicted Label  \n",
       "0             joy  \n",
       "1         sadness  \n",
       "2         sadness  \n",
       "3             joy  \n",
       "4             joy  \n",
       "5         sadness  \n",
       "6         sadness  \n",
       "7             joy  \n",
       "8             joy  \n",
       "9             joy  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a dataframe with the predictions and the text and the labels\n",
    "\n",
    "results_nofinetuning = trainer_nofinetuning.predict(tokenized_dataset[\"test\"])\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_labels_nofinetuning = np.argmax(results_nofinetuning.predictions, axis=1)\n",
    "\n",
    "# Get the text of the examples\n",
    "texts = [item[\"text\"] for item in tokenized_dataset[\"test\"]]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Text\": texts,\n",
    "    \"True Label\": results_nofinetuning.label_ids,\n",
    "    \"Predicted Label\": predicted_labels_nofinetuning\n",
    "})\n",
    "\n",
    "# Map label indices to actual emotions\n",
    "label_map = {0: \"sadness\", 1: \"joy\", 2: \"love\", 3: \"anger\", 4: \"fear\", 5: \"surprise\"}\n",
    "df[\"True Label\"] = df[\"True Label\"].map(label_map)\n",
    "df[\"Predicted Label\"] = df[\"Predicted Label\"].map(label_map)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873b37cb-ae51-4529-af77-293ed6a7b820",
   "metadata": {},
   "source": [
    "### Prepare for Parameter Efficient Fine Tuning of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "929eb0a8-3dc4-4799-86b3-ce08e729ac1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravverma/anaconda3/envs/udacitygenai/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=6,\n",
    "    id2label={0: \"sadness\", 1: \"joy\", 2: \"love\", 3: \"anger\", 4: \"fear\", 5: \"surprise\"},\n",
    "    label2id={\"sadness\": 0, \"joy\": 1, \"love\": 2, \"anger\": 3, \"fear\": 4, \"surprise\": 5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d106fb28-8c82-4bcc-aaaf-f36e1cd1cc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 147,456 || all params: 67,105,542 || trainable%: 0.2197\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA for parameter-efficient fine-tuning\n",
    "config = LoraConfig(\n",
    "    r=4,               # Rank of the adaptation matrix\n",
    "    lora_alpha=32,     # Alpha scaling factor\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"],  # Adjust target modules for DistilBERT\n",
    "    lora_dropout=0.1   # Dropout rate\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7416b2d-9dad-4c98-9557-22837c060f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the parameters in the target modules\n",
    "for name, param in model.named_parameters():\n",
    "    if any(target in name for target in config.target_modules):\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf57da4c-97d4-4e31-9cd7-06893bfcf4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tokenized_dataset[\"train\"].features[\"label\"].names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047652b8-95ec-4f5f-8e42-df7f2eb4f1f9",
   "metadata": {},
   "source": [
    "### Train the model with PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9b6a2-dbb5-49db-ae95-d234ce154b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Trainer for training (with PEFT)\n",
    "trainer_with_peft = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/emotions_with_peft\",\n",
    "        learning_rate=2e-5,\n",
    "        label_names=['labels'],\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=64,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        logging_dir='./data/emotions_with_peft/logs_with_peft',\n",
    "        logging_steps=100,\n",
    "        ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model with PEFT\n",
    "trainer_with_peft.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e23fa28-2e6b-4d92-a4a5-efa7ad4215d0",
   "metadata": {},
   "source": [
    "### Evaluate the PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04572c93-acdf-44af-90e0-6eeaef5a1bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5748061537742615,\n",
       " 'eval_accuracy': 0.865,\n",
       " 'eval_runtime': 1.7542,\n",
       " 'eval_samples_per_second': 1140.112,\n",
       " 'eval_steps_per_second': 18.242,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_with_peft.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2663f76-c065-4cae-954d-c5717b9a98fa",
   "metadata": {},
   "source": [
    "### Save the Trained PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3411bae0-c211-41be-a0b8-97072b2c69e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravverma/anaconda3/envs/udacitygenai/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./savedpeftmodel/tokenizer_config.json',\n",
       " './savedpeftmodel/special_tokens_map.json',\n",
       " './savedpeftmodel/vocab.txt',\n",
       " './savedpeftmodel/added_tokens.json',\n",
       " './savedpeftmodel/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "trainer_with_peft.save_model(\"./savedpeftmodel\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"./savedpeftmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7963449a-6826-49d5-84c1-d156e7e3b9ae",
   "metadata": {},
   "source": [
    "### Load the saved PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a726f68-088a-4dbb-9a89-226d29fc6ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./savedpeftmodel\")\n",
    "\n",
    "# Load the PEFT model\n",
    "model_peft = AutoPeftModelForSequenceClassification.from_pretrained(\"./savedpeftmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bfebe7-1383-48f5-88f2-11f88b6ea834",
   "metadata": {},
   "source": [
    "### Evaluate the saved and loaded PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b81f96cd-e8fa-4b5a-b479-ac50e274376b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravverma/anaconda3/envs/udacitygenai/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44630372524261475, 'eval_accuracy': 0.275, 'eval_runtime': 13.5025, 'eval_samples_per_second': 148.12, 'eval_steps_per_second': 18.515}\n"
     ]
    }
   ],
   "source": [
    "# Create the Trainer for evaluation\n",
    "trainer_eval = Trainer(\n",
    "    model=model_peft,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "results = trainer_eval.evaluate(tokenized_dataset[\"validation\"])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36dbe01-1f4f-4eb1-af0e-7693fb18b688",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We can see that in this case, \n",
    "- Model with no finetuning has an accuracy of 48.75%\n",
    "- Model with PEFT had an accuracy of 86.5%\n",
    "- However, once this trained model was saved and reloaded, its accuracy drops drastially to 27.5% . Probably there's an issue with the Huggingface library AutoPeftModelForSequenceClassification in properly loading the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9121032-8f4f-4612-8bd3-d8fce2b83717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
