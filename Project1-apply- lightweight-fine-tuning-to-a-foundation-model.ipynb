{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a226087a-e2d6-4def-b33c-47e7d9a114fa",
   "metadata": {},
   "source": [
    "# Project: Apply Lightweight Fine-Tuning to a Foundation Model\n",
    "\n",
    "In this project, have used the Emotions dataset [https://huggingface.co/datasets/dair-ai/emotion]\n",
    "\n",
    "The dataset contains:\n",
    "- Text column which has statements that need to be classified as one of the emotions\n",
    "- Label column which has labels describing the emotion as sadness (0), joy (1), love (2), anger (3), fear (4), surprise (5)  \n",
    "\n",
    "Initially the foundation model BERT sentiment classifier (DistilBERT) will be used, without finetuning the base model parameters and then its performance will be evaluated. Then, the model will be trained using PEFT approach to finetune the parameters and the performance will be evaluated. Finally, we will compare the performance of the model without parameter finetuning and with parameter efficient finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf66c72-aab1-4ab3-922b-85f40ae726a5",
   "metadata": {},
   "source": [
    "### Importing Libraries and Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f32916fb-c265-44ec-9785-d0821b1acf73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb13b68-7bfa-4610-ace5-01258a421f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 16000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the split configuration of the Emotion dataset\n",
    "dataset = load_dataset(\"emotion\", \"split\")\n",
    "\n",
    "# View the dataset characteristics\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df8bad3-ee73-49f4-ad90-a917cea605ab",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894547a0-1b32-4beb-b231-adf837e4ee1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravverma/anaconda3/envs/udacitygenai/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'i didnt feel humiliated', 'label': 0, 'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Inspect the tokenized dataset\n",
    "print(tokenized_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8729228-113e-4f21-9a44-80c567ccd626",
   "metadata": {},
   "source": [
    "### Load and Setup the model without finetuning the base model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "877a5167-85dc-4a33-af4c-6ae86e6d3b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=6, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained model for sequence classification\n",
    "model_nofinetuning = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=6,\n",
    "    id2label={0: \"sadness\", 1: \"joy\", 2: \"love\", 3: \"anger\", 4: \"fear\", 5: \"surprise\"},\n",
    "    label2id={\"sadness\": 0, \"joy\": 1, \"love\": 2, \"anger\": 3, \"fear\": 4, \"surprise\": 5},\n",
    ")\n",
    "\n",
    "#Freeze all the parameters of base model\n",
    "for param in model_nofinetuning.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_nofinetuning.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe8f559b-f7d0-4c0b-bce2-45193228ed78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_nofinetuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6979ba5b-26f1-487c-8cd4-eb70f631a39d",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ea80e61-d01c-4e18-a41e-9614ace0c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy Metric to be supplied to trainer\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15fb1405-bd80-4984-bcb0-5e97c5d01e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravverma/anaconda3/envs/udacitygenai/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 00:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.574200</td>\n",
       "      <td>1.548101</td>\n",
       "      <td>0.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.524100</td>\n",
       "      <td>1.514742</td>\n",
       "      <td>0.485000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.504200</td>\n",
       "      <td>1.503843</td>\n",
       "      <td>0.486500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=1.5450640665690105, metrics={'train_runtime': 51.5627, 'train_samples_per_second': 930.905, 'train_steps_per_second': 14.545, 'total_flos': 703485182771712.0, 'train_loss': 1.5450640665690105, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Trainer\n",
    "trainer_nofinetuning = Trainer(\n",
    "    model=model_nofinetuning,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/emotions\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=64,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy = \"epoch\",\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        logging_dir='.data/emotions/logs_no_finetuning',\n",
    "        logging_steps=100,\n",
    "        ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer_nofinetuning.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d885ba91-7649-4879-8c72-2907240b9ff7",
   "metadata": {},
   "source": [
    "### Evaluate the Model without Finetuning and visualize its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03bcee07-b4aa-405c-8f42-d98bfb7691d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.503842830657959,\n",
       " 'eval_accuracy': 0.4865,\n",
       " 'eval_runtime': 1.4723,\n",
       " 'eval_samples_per_second': 1358.388,\n",
       " 'eval_steps_per_second': 21.734,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_nofinetuning.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d943df7-d1e0-484c-8e20-578f25277892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>True Label</th>\n",
       "      <th>Predicted Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im feeling rather rotten so im not very ambiti...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im updating my blog because i feel shitty</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i never make her separate from me because i do...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i left with my bouquet of red and yellow tulip...</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was feeling a little vain when i did this one</td>\n",
       "      <td>sadness</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i cant walk into a shop anywhere where i do no...</td>\n",
       "      <td>fear</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i felt anger when at the end of a telephone call</td>\n",
       "      <td>anger</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i explain why i clung to a relationship with a...</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i like to have the same breathless feeling as ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i jest i feel grumpy tired and pre menstrual w...</td>\n",
       "      <td>anger</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text True Label  \\\n",
       "0  im feeling rather rotten so im not very ambiti...    sadness   \n",
       "1          im updating my blog because i feel shitty    sadness   \n",
       "2  i never make her separate from me because i do...    sadness   \n",
       "3  i left with my bouquet of red and yellow tulip...        joy   \n",
       "4    i was feeling a little vain when i did this one    sadness   \n",
       "5  i cant walk into a shop anywhere where i do no...       fear   \n",
       "6   i felt anger when at the end of a telephone call      anger   \n",
       "7  i explain why i clung to a relationship with a...        joy   \n",
       "8  i like to have the same breathless feeling as ...        joy   \n",
       "9  i jest i feel grumpy tired and pre menstrual w...      anger   \n",
       "\n",
       "  Predicted Label  \n",
       "0             joy  \n",
       "1         sadness  \n",
       "2         sadness  \n",
       "3             joy  \n",
       "4             joy  \n",
       "5         sadness  \n",
       "6         sadness  \n",
       "7             joy  \n",
       "8             joy  \n",
       "9             joy  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a dataframe with the predictions and the text and the labels\n",
    "\n",
    "results_nofinetuning = trainer_nofinetuning.predict(tokenized_dataset[\"test\"])\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_labels_nofinetuning = np.argmax(results_nofinetuning.predictions, axis=1)\n",
    "\n",
    "# Get the text of the examples\n",
    "texts = [item[\"text\"] for item in tokenized_dataset[\"test\"]]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Text\": texts,\n",
    "    \"True Label\": results_nofinetuning.label_ids,\n",
    "    \"Predicted Label\": predicted_labels_nofinetuning\n",
    "})\n",
    "\n",
    "# Map label indices to actual emotions\n",
    "label_map = {0: \"sadness\", 1: \"joy\", 2: \"love\", 3: \"anger\", 4: \"fear\", 5: \"surprise\"}\n",
    "df[\"True Label\"] = df[\"True Label\"].map(label_map)\n",
    "df[\"Predicted Label\"] = df[\"Predicted Label\"].map(label_map)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873b37cb-ae51-4529-af77-293ed6a7b820",
   "metadata": {},
   "source": [
    "### Prepare for Parameter Efficient Fine Tuning of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "929eb0a8-3dc4-4799-86b3-ce08e729ac1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravverma/anaconda3/envs/udacitygenai/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# # Load the pre-trained model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=6,\n",
    "    id2label={0: \"sadness\", 1: \"joy\", 2: \"love\", 3: \"anger\", 4: \"fear\", 5: \"surprise\"},\n",
    "    label2id={\"sadness\": 0, \"joy\": 1, \"love\": 2, \"anger\": 3, \"fear\": 4, \"surprise\": 5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d106fb28-8c82-4bcc-aaaf-f36e1cd1cc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 147,456 || all params: 67,105,542 || trainable%: 0.2197\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA for parameter-efficient fine-tuning\n",
    "config = LoraConfig(\n",
    "    r=4,               # Rank of the adaptation matrix\n",
    "    lora_alpha=32,     # Alpha scaling factor\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"],  # Adjust target modules for DistilBERT\n",
    "    lora_dropout=0.1   # Dropout rate\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7416b2d-9dad-4c98-9557-22837c060f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the parameters in the target modules\n",
    "for name, param in model.named_parameters():\n",
    "    if any(target in name for target in config.target_modules):\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf57da4c-97d4-4e31-9cd7-06893bfcf4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tokenized_dataset[\"train\"].features[\"label\"].names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047652b8-95ec-4f5f-8e42-df7f2eb4f1f9",
   "metadata": {},
   "source": [
    "### Train the model with PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03ea3cf8-f6a3-4696-88d3-c2fca49333ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravverma/anaconda3/envs/udacitygenai/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 02:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.203500</td>\n",
       "      <td>0.921001</td>\n",
       "      <td>0.732500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.751400</td>\n",
       "      <td>0.663432</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.655300</td>\n",
       "      <td>0.600001</td>\n",
       "      <td>0.834500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravverma/anaconda3/envs/udacitygenai/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/sauravverma/anaconda3/envs/udacitygenai/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=0.9353773651123047, metrics={'train_runtime': 174.5644, 'train_samples_per_second': 274.97, 'train_steps_per_second': 4.296, 'total_flos': 705890645475840.0, 'train_loss': 0.9353773651123047, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Trainer for the second training (with PEFT)\n",
    "trainer_with_peft = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/emotions_with_peft\",\n",
    "        learning_rate=2e-5,\n",
    "        label_names=['labels'],\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=64,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        logging_dir='./data/emotions_with_peft/logs_with_peft',\n",
    "        logging_steps=100,\n",
    "        ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model with PEFT\n",
    "trainer_with_peft.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bfebe7-1383-48f5-88f2-11f88b6ea834",
   "metadata": {},
   "source": [
    "### Evaluate the Model with PEFT and visualize its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b81f96cd-e8fa-4b5a-b479-ac50e274376b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.600001335144043,\n",
       " 'eval_accuracy': 0.8345,\n",
       " 'eval_runtime': 1.8378,\n",
       " 'eval_samples_per_second': 1088.275,\n",
       " 'eval_steps_per_second': 17.412,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_with_peft.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97a5b83c-7172-45a5-916e-ca18e5963a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>True Label</th>\n",
       "      <th>Predicted Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im feeling rather rotten so im not very ambiti...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im updating my blog because i feel shitty</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i never make her separate from me because i do...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i left with my bouquet of red and yellow tulip...</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was feeling a little vain when i did this one</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i cant walk into a shop anywhere where i do no...</td>\n",
       "      <td>fear</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i felt anger when at the end of a telephone call</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i explain why i clung to a relationship with a...</td>\n",
       "      <td>joy</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i like to have the same breathless feeling as ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i jest i feel grumpy tired and pre menstrual w...</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text True Label  \\\n",
       "0  im feeling rather rotten so im not very ambiti...    sadness   \n",
       "1          im updating my blog because i feel shitty    sadness   \n",
       "2  i never make her separate from me because i do...    sadness   \n",
       "3  i left with my bouquet of red and yellow tulip...        joy   \n",
       "4    i was feeling a little vain when i did this one    sadness   \n",
       "5  i cant walk into a shop anywhere where i do no...       fear   \n",
       "6   i felt anger when at the end of a telephone call      anger   \n",
       "7  i explain why i clung to a relationship with a...        joy   \n",
       "8  i like to have the same breathless feeling as ...        joy   \n",
       "9  i jest i feel grumpy tired and pre menstrual w...      anger   \n",
       "\n",
       "  Predicted Label  \n",
       "0         sadness  \n",
       "1         sadness  \n",
       "2         sadness  \n",
       "3             joy  \n",
       "4         sadness  \n",
       "5            fear  \n",
       "6           anger  \n",
       "7         sadness  \n",
       "8             joy  \n",
       "9           anger  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a dataframe with the predictions and the text and the labels\n",
    "\n",
    "results_peft = trainer_with_peft.predict(tokenized_dataset[\"test\"])\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_labels = np.argmax(results_peft.predictions, axis=1)\n",
    "\n",
    "# Get the text of the examples\n",
    "texts = [item[\"text\"] for item in tokenized_dataset[\"test\"]]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Text\": texts,\n",
    "    \"True Label\": results_peft.label_ids,\n",
    "    \"Predicted Label\": predicted_labels\n",
    "})\n",
    "\n",
    "# Map label indices to actual emotions\n",
    "label_map = {0: \"sadness\", 1: \"joy\", 2: \"love\", 3: \"anger\", 4: \"fear\", 5: \"surprise\"}\n",
    "df[\"True Label\"] = df[\"True Label\"].map(label_map)\n",
    "df[\"Predicted Label\"] = df[\"Predicted Label\"].map(label_map)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36dbe01-1f4f-4eb1-af0e-7693fb18b688",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We can clearly see that the model with performance efficient fine tuning has a much higher accuracy (over 83%) compared to the model with no finetuning (over 48%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a330a04-e9db-444e-8928-6987821a650e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
